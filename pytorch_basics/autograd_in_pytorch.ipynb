{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb769f8f-7b78-48b2-8def-01fcc369bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e263e23-67a8-4bff-82d7-c71c8e9ca119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7324, 0.2427, 0.9295, 0.0753, 0.8384]) None False\n"
     ]
    }
   ],
   "source": [
    "## create a normal tensor\n",
    "## By default the tensor has no gradient storage.\n",
    "\n",
    "x = torch.rand(5)\n",
    "print(x, x.grad, x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689a79c-b791-4663-9822-372603a1a2c8",
   "metadata": {},
   "source": [
    "## When final output is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42df27b4-c3ff-4909-90e5-95959d2c479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5811, 0.4933, 0.3618, 0.4305, 0.6741], requires_grad=True)\n",
      "tensor([2.5811, 2.4933, 2.3618, 2.4305, 2.6741], grad_fn=<AddBackward0>)\n",
      "tensor(12.6058, grad_fn=<MeanBackward0>)\n",
      "tensor([2.0649, 1.9946, 1.8894, 1.9444, 2.1393])\n"
     ]
    }
   ],
   "source": [
    "## we need to set requires_grad value, to store the gradient\n",
    "## it generates a computational graph of all the opeartions\n",
    "## carried on that tensor, and calculates gradient.\n",
    "\n",
    "x = torch.rand(5, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "## sequence of operations\n",
    "y = x + 2\n",
    "z = y * y * 2\n",
    "z = z.mean()\n",
    "\n",
    "## this will add gradient computation function to each of the variables\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "## .backward() will trigger the gradient propagation\n",
    "z.backward() ## calculate dz/dx\n",
    "\n",
    "## propagate till x (the input)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf387a3c-99df-4365-9afd-4e2c13c4ad65",
   "metadata": {},
   "source": [
    "## When final output is a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a6fdb8-1cf8-4884-b745-ad6e982239c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6953, 0.3307, 0.0088, 0.0932, 0.3140], requires_grad=True)\n",
      "tensor([2.6953, 2.3307, 2.0088, 2.0932, 2.3140], grad_fn=<AddBackward0>)\n",
      "tensor([14.5297, 10.8647,  8.0704,  8.7628, 10.7094], grad_fn=<MulBackward0>)\n",
      "tensor([10.7814,  9.3230,  8.0351,  8.3727,  9.2561])\n"
     ]
    }
   ],
   "source": [
    "## we need to set requires_grad value, to store the gradient\n",
    "## it generates a computational graph of all the opeartions\n",
    "## carried on that tensor, and calculates gradient.\n",
    "\n",
    "x = torch.rand(5, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "## sequence of operations\n",
    "y = x + 2\n",
    "z = y * y * 2\n",
    "\n",
    "## this will add gradient computation function to each of the variables\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "## .backward() will trigger the gradient propagation\n",
    "## but when z is a vector, we need to pass a vector when calling .backward()\n",
    "## it should be of the same size as z.\n",
    "\n",
    "v = torch.ones(5, dtype=torch.float32)\n",
    "z.backward(v) ## calculate dz/dx\n",
    "\n",
    "## propagate till x (the input)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d2fe1-1df1-43fc-9e7c-007f1f079fff",
   "metadata": {},
   "source": [
    "## When we do not need to compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef1c0ab-6316-4d17-857b-70a0a4468aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.5680,  0.0068, -0.6652,  1.9122, -0.7000], requires_grad=True)\n",
      "tensor([ 2.5680,  0.0068, -0.6652,  1.9122, -0.7000])\n",
      "------------------------------\n",
      "tensor([ 1.0561, -0.1023,  2.0452,  0.4143,  0.4173], requires_grad=True)\n",
      "tensor([ 1.0561, -0.1023,  2.0452,  0.4143,  0.4173])\n",
      "------------------------------\n",
      "tensor([ 0.6888, -0.9357, -1.0613, -1.4771,  0.6907], requires_grad=True)\n",
      "tensor([2.6888, 1.0643, 0.9387, 0.5229, 2.6907])\n"
     ]
    }
   ],
   "source": [
    "## Example: when we need to update the weights with the gradient.\n",
    "\n",
    "## There are 3 ways to do that.\n",
    "\n",
    "## 1. .requires_grad_()\n",
    "\n",
    "x = torch.randn(5, requires_grad = True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "print(\"-\"*30)\n",
    "\n",
    "## 2. .detach()\n",
    "## It creates a new copy of no gradient tensor.\n",
    "\n",
    "x = torch.randn(5, requires_grad = True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)\n",
    "print(\"-\"*30)\n",
    "\n",
    "## 3. with torch.no_grad():\n",
    "\n",
    "x = torch.randn(5, requires_grad = True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5255b1-cb7f-44f8-8354-416ed6d28755",
   "metadata": {},
   "source": [
    "## Empty gradient after each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59da4d7e-5d9f-4f44-a505-ff3267f5b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n",
      "------------------------------\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "## Pytorch .grad attribute accumulates gradient for each iteration.\n",
    "## Example:\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum() # to get a scalar value\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "## For proper execution and not accumulating the grads\n",
    "## zero out the grad for each iteration.\n",
    "\n",
    "print(\"-\"*30)\n",
    "\n",
    "weights.grad.zero_()\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum() # to get a scalar value\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246e420-c05d-45b6-9690-f8287f212f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
