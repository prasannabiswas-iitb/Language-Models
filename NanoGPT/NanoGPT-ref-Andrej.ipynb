{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NanoGPT : Code from scratch**\n",
        "\n",
        "This is part of understanding \"transformer\" architecture (Decoder in this case) by implementing it from the scratch without using prefedined decoder architecture.\n",
        "\n",
        "**Credit**: Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out (https://www.youtube.com/watch?v=kCc8FmEb1nY)"
      ],
      "metadata": {
        "id": "R10EFvzrv0Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "iWKtdkLQGw49"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXBVblXQt23p",
        "outputId": "7e5eef9d-6549-4ed5-9952-e8cac912f651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-24 17:53:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-12-24 17:53:10 (56.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downlaoding tiny shakespeare dataset as used in the video\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper-parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 64\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Loading the dataset\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Build a tokenizer\n",
        "## Collect all the unique characters ocurring in the dataset.\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "## Create a mapping from characters to integers and vice-versa\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "## Create lambda functions\n",
        "## function_name = lambda variable: operation\n",
        "### take input as string and return a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "### take list of integers as input and return a string\n",
        "decode = lambda l: ' '.join([itos[i] for i in l])\n",
        "\n",
        "# Creating training and test splits\n",
        "data = torch.tensor((encode(text)), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Load data\n",
        "def get_batch(split):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  # generate random integers from the torch.randint(low=0, high=(data-blocksize)\n",
        "  # , output tensor shape = (batchsize,1) )\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "# Implementing single head attention\n",
        "class Head(nn.Module):\n",
        "  \"\"\" single head attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(in_features=n_embd, out_features=head_size, bias=False)\n",
        "    self.query = nn.Linear(in_features=n_embd, out_features=head_size, bias=False)\n",
        "    self.value = nn.Linear(in_features=n_embd, out_features=head_size, bias=False)\n",
        "    '''\n",
        "    register_buffer(name, tensor, persistent=True)[SOURCE]\n",
        "    Adds a buffer to the module.\n",
        "    This is typically used to register a buffer that should not to be considered a model parameter.\n",
        "    For example, BatchNorm’s running_mean is not a parameter, but is part of the module’s state.\n",
        "    '''\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    # Compute the affinities\n",
        "    wei = q @ k.transpose(-2,-1) * (C**-0.5)\n",
        "    ''' tensor.masked_fill(mask, value) '''\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "# Extending it to Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.projection = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.projection(out))\n",
        "    return out\n",
        "\n",
        "# Implementing a feed forward network to apply after attention.\n",
        "# (inverse bottleneck)\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "# Implementing a decoder block\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer decoder block: attention followed by linear layer \"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "# Implementing transformer decoder model\n",
        "class Decoder(nn.Module):\n",
        "  \"\"\" Transformer decoder architecture \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # B,T,C\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "    # Same position values for each batch\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx (B,T)\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to (block_size) length as position_ids till that is available\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus on last time-step OR last token\n",
        "      logits = logits[:, -1, :] # B,T,C becomes B,C\n",
        "      # apply softmax for probabilities\n",
        "      probs = F.softmax(logits, dim=-1) #B,C\n",
        "      # Instead of picking top, sample from distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #B,1\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #B,T+1\n",
        "    return idx\n",
        "\n",
        "# Running the decoder block\n",
        "model = Decoder()\n",
        "model = model.to(device)\n",
        "\n",
        "# Num of params\n",
        "print(sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# Create a optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Calculate training and validation loss\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "# Training step\n",
        "for iter in range(max_iters):\n",
        "  # every interval eval training and validation loss\n",
        "  if iter % eval_interval == 0 or iter == max_iters-1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate loss\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "bos = torch.zeros((1,1), dtype = torch.long, device=device)\n",
        "print(decode(model.generate(bos, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdP4lTVc42Up",
        "outputId": "ae806d43-9bab-45cc-fb1d-4bc80e2a06f0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "812609\n",
            "step 0: train loss 4.3210, val loss 4.3210\n",
            "step 100: train loss 2.5376, val loss 2.5389\n",
            "step 200: train loss 2.4394, val loss 2.4524\n",
            "step 300: train loss 2.3774, val loss 2.3876\n",
            "step 400: train loss 2.3189, val loss 2.3355\n",
            "step 500: train loss 2.2658, val loss 2.2863\n",
            "step 600: train loss 2.2172, val loss 2.2390\n",
            "step 700: train loss 2.1687, val loss 2.1933\n",
            "step 800: train loss 2.1125, val loss 2.1471\n",
            "step 900: train loss 2.0755, val loss 2.1304\n",
            "step 1000: train loss 2.0397, val loss 2.1158\n",
            "step 1100: train loss 2.0025, val loss 2.0787\n",
            "step 1200: train loss 1.9650, val loss 2.0604\n",
            "step 1300: train loss 1.9308, val loss 2.0375\n",
            "step 1400: train loss 1.9184, val loss 2.0239\n",
            "step 1500: train loss 1.8933, val loss 2.0084\n",
            "step 1600: train loss 1.8676, val loss 1.9836\n",
            "step 1700: train loss 1.8428, val loss 1.9716\n",
            "step 1800: train loss 1.8264, val loss 1.9780\n",
            "step 1900: train loss 1.8059, val loss 1.9282\n",
            "step 2000: train loss 1.7951, val loss 1.9454\n",
            "step 2100: train loss 1.7931, val loss 1.9328\n",
            "step 2200: train loss 1.7685, val loss 1.9149\n",
            "step 2300: train loss 1.7611, val loss 1.9018\n",
            "step 2400: train loss 1.7495, val loss 1.8990\n",
            "step 2500: train loss 1.7555, val loss 1.8845\n",
            "step 2600: train loss 1.7280, val loss 1.8884\n",
            "step 2700: train loss 1.7090, val loss 1.8589\n",
            "step 2800: train loss 1.6975, val loss 1.8562\n",
            "step 2900: train loss 1.6941, val loss 1.8655\n",
            "step 3000: train loss 1.6985, val loss 1.8627\n",
            "step 3100: train loss 1.7102, val loss 1.8736\n",
            "step 3200: train loss 1.6721, val loss 1.8543\n",
            "step 3300: train loss 1.6799, val loss 1.8370\n",
            "step 3400: train loss 1.6633, val loss 1.8281\n",
            "step 3500: train loss 1.6603, val loss 1.8478\n",
            "step 3600: train loss 1.6459, val loss 1.8336\n",
            "step 3700: train loss 1.6561, val loss 1.8174\n",
            "step 3800: train loss 1.6523, val loss 1.8166\n",
            "step 3900: train loss 1.6444, val loss 1.8315\n",
            "step 4000: train loss 1.6296, val loss 1.7972\n",
            "step 4100: train loss 1.6278, val loss 1.7905\n",
            "step 4200: train loss 1.6115, val loss 1.7977\n",
            "step 4300: train loss 1.6146, val loss 1.7860\n",
            "step 4400: train loss 1.6113, val loss 1.7814\n",
            "step 4500: train loss 1.5993, val loss 1.7779\n",
            "step 4600: train loss 1.5997, val loss 1.7742\n",
            "step 4700: train loss 1.5978, val loss 1.7738\n",
            "step 4800: train loss 1.5869, val loss 1.7727\n",
            "step 4900: train loss 1.5989, val loss 1.7566\n",
            "step 4999: train loss 1.5850, val loss 1.7530\n",
            "\n",
            " \n",
            " K I N G   R I C H A R D   I I : \n",
            " S h a l l   b e   b e   m a d e   t o   b u b l o o d :   a n d   O - d e a t h   a g a i n \n",
            " h e r   r a b i l i e n c y   o f   h i s   b a r e d   l a s c a u e s u r e . \n",
            " P r o c e ,   n o t   t o   z u c h m p e n o r ,   L a d y f u l   I   c a l l   m y   f o l i c e ,   a n d   i f   e y e ,   I   i n   l a t t e m a n ;   o r   t h e   d o e \n",
            " W i l l   m a y   i s   w a n   t w e l c o m   d i s e   o f   t h e   r e c e c r i o y :   t h e r e ' s   s p e a k   y o u   l o r d . \n",
            " I   c a m e   a t   i g u l l y   b e t t y   w o u l d   t h a t \n",
            " T o   W i l l o   w h a t   e v i l y   w e   a r e   d o s t   w i f e   e y e ; \n",
            " B u t   p o o c e l y   h i s   b u t t y   a n d   t h r u s t   f i r   a r m ; \n",
            " T h e n   w h i t   t e y e s   M o w b r a y   M a d e r e r :   I   h a v e   a   f u l l   a n d   t i m b u i s : \n",
            " S h o u l d   y o u   a r e   s t o v e i n   c o u r r a c e   t e y e s , - - \n",
            " \n",
            " S I C I N I U S : \n",
            " N o w ,   n o   t o   P o m e ,   m y   l i f e   c o m e ,   m y   m a s t e r   a n d   s e v i r e n c e ; \n",
            " B u t   w i t h   r e a s o n ;   s t a n g   i n   G o d ' s   w e r e   o w e . \n",
            " \n",
            " T Y R R R E : \n",
            " I   m a y s t   a t c a s o n . \n",
            " \n",
            " A E E i l : \n",
            " G o   l o r d i k e s . \n",
            " \n",
            " L A D Y   P R I : \n",
            " T h i s   l i f e   m o u t h s e   t o   o u r   f a r e e d ;   H o w \n",
            " O n   t u m e   h i s   r a f e   i t   a s   n y i n c e ,   I   s h a l l   n o w   t h e n   i n   R a c i o , \n",
            " T o   m a k e   h e a v e n   a b k e t   a g a i n s t   u n c e s t r e s   s l e e . \n",
            " \n",
            " C L O R E O : \n",
            " C o m e ,   w e l c o m e   t h i s   h e r c i v e d \n",
            " F o u r s w o r .   I   k n o w   t h e   n o t h u n g   C a l i t y \n",
            " H e   w i l l   o f   t i m e   i t   o u n   f i n c e   t o   m o r e : \n",
            " I   t a k e   i s   b e   t h a n   t h y   o u g h t   E i B l i s h   k i n g   I   s o   h a v e   a p t i b e d \n",
            " B y   t h i s   i n   o w n   t o   g l e t   u s   y o u r   w i t h   e r e   r o a n e   i t   w e   h a f l s ,   I   h u s   n e e d ,   w h a t   n u r s e   i '   t h e   d i d   d i s a ; \n",
            " T h a n   y o u   w e r e   s h o u l d   m e n ,   m e r e   t h i n e s   a n d   e n o o r s i n g   t h i n - s o n e , \n",
            " T h e   W a r w i c k   h i s   c h i e r   f r i e n d !   D i s w o   a n I   h a v e   a   c a n i e n c e \n",
            " A n d   f i n d   o a   f e e n c e   i t   b l a m e n t   t e l l   t r u e   e i r e   b r o t h e r i a l . \n",
            " \n",
            " D U K E   V I N C E I O : \n",
            " I t   d i d   t h o u   f o r   H e n r y ' s   m o o r e l ; \n",
            " F o r   t h e r e   c o l i t y   e n d   j u s t   ' W e   t o w a r d   m u r d e r , \n",
            " S h a l l   a n d   G i v e   i t   i s   c a p t i e n c l e s ! \n",
            " \n",
            " W A R W I C K : \n",
            " H o w   s i c c h a r s   b e g u t ? \n",
            " \n",
            " K I N G   H E N R Y   V I : \n",
            " F r o m   H o n v e ' s   e l s e   n o w ;   I   t h i n k e   n u m b o r m   T a k e   T h e   p o m p e d e r ! \n",
            " H o w   s c o u l d   o f   R o m e o :   w h o l e   t o   t e l l   s m o r r o w . \n",
            " \n",
            " H E R M I O : \n",
            " \n",
            " M E N E R O : \n",
            " W e   a s   I f   h e r   d i d   k i s   I   s c a l l :   f o o t   m y   b e e n ; \n",
            " L e t   s e e   w e   h u n s t   a r e   g e n   w a l l   p r e v e n g e d   o f   t r y i n g \n",
            " N o   h i m   h i m   w e r e   d e s e r m e   r i s h o u   a r w i l l . \n",
            " \n",
            " B U C K I N G H A M : \n",
            " O ,   c o u r t   h i s   i n   o a t h - - - \n",
            " I S \n",
            " ' C i t   T a k e   t w a y : \n",
            " A n d   t h e n - s h a l l   c a l l   t h i s   f a t h e r ,   E d w a r d ' s   c i t s ,   o f   m o n e   ' t w o u l d   t a l k   w e   t w o   p e e c h   m o t h e r ? \n",
            " B u t   I   h a v e   b u t   I   l i k e   i n   t o   l i f e ? \n",
            " \n",
            " C O M I N I U S : \n",
            " Y o u   d o   t h e   a w o u g h t :   G o o d ' s   n u r s e , \n",
            " H y   p o o r   a n d   g r i a v o u s   a n d   J o m e   a t t a b l e , \n",
            " W i t h ,   J u l i o y   p a r f o r t :   I   w i l l   d o   n o w ? \n",
            " \n",
            " L U C I O : \n",
            " F o r   m e   o t e n d   h e r   d e g o , \n",
            " ' T i s\n"
          ]
        }
      ]
    }
  ]
}